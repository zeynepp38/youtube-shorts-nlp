# -*- coding: utf-8 -*-
"""analiz.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hM0YQLtqJMY_NjdtGZgZBIxGz36oPCYB
"""

from zipfile import ZipFile

with ZipFile("/content/archive.zip", "r") as zip_ref:
    zip_ref.extractall("/content/dataset")

!ls /content/dataset

!ls /content/dataset

!ls /content/dataset/default
!ls /content/dataset/gaming
!ls /content/dataset/movies
!ls /content/dataset/music

import pandas as pd
from pathlib import Path

base_path = Path("/content/dataset")
categories = ["default", "gaming", "movies", "music"]

dfs = []

for category in categories:
    for csv_file in (base_path / category).glob("*.csv"):
        df = pd.read_csv(csv_file)

        # Dosya isminden tarihi çekiyoruz: default_20240703.csv → 2024-07-03
        date_str = csv_file.stem.split("_")[1]
        df["snapshot_date"] = pd.to_datetime(date_str, format="%Y%m%d", errors="coerce")
        df["category"] = category

        dfs.append(df)

all_df = pd.concat(dfs, ignore_index=True)

print("Toplam satır:", len(all_df))
all_df.head()

import os
import pandas as pd

base_path = "/content/dataset"
dfs = []

for category in ["default", "gaming", "movies", "music"]:
    folder = os.path.join(base_path, category)
    for file in os.listdir(folder):
        if file.endswith(".csv"):
            df_temp = pd.read_csv(os.path.join(folder, file))
            df_temp["category"] = category
            dfs.append(df_temp)

df = pd.concat(dfs, ignore_index=True)
print(df["category"].value_counts())

all_df.columns

df = all_df.copy()

# Sadece Shorts
df = df[df["isShort"] == True]

# Views kolonunu sayıya çevir (string gelmiş olabilir)
df["views"] = (
    df["views"]
    .astype(str)
    .str.replace(",", "", regex=False)
)

df["views"] = pd.to_numeric(df["views"], errors="coerce")

print("Shorts sayısı:", len(df))
df[["title", "category", "views"]].head()

df.groupby("category")["views"].mean().sort_values(ascending=False)

df.sort_values("views", ascending=False)[["title", "category", "views"]].head(15)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

texts = df["title"].fillna("").astype(str)

vectorizer = TfidfVectorizer(
    stop_words="english",
    max_features=5000,
    ngram_range=(1,2)  # tek kelime + ikili kelime grupları
)

X = vectorizer.fit_transform(texts)

kmeans = KMeans(n_clusters=6, random_state=42, n_init="auto")
df["topic_cluster"] = kmeans.fit_predict(X)

df[["title", "category", "views", "topic_cluster"]].sample(10)

import numpy as np

terms = vectorizer.get_feature_names_out()

for i in range(6):
    center = kmeans.cluster_centers_[i]
    top_terms = [terms[ind] for ind in center.argsort()[-12:]]
    print(f"\nCluster {i}: {top_terms}")

df.groupby("topic_cluster")["views"].mean().sort_values(ascending=False)

from collections import Counter
import re

words = " ".join(df.sort_values("views", ascending=False).head(2000)["title"]).lower()
tokens = re.findall(r"[a-zA-Z]{3,}", words)

Counter(tokens).most_common(30)

df.to_csv("/content/shorts_enriched.csv", index=False)

for i in sorted(df["topic_cluster"].unique()):
    print("\nCLUSTER", i)
    print(df[df["topic_cluster"] == i]["title"].head(5).to_list())

df.groupby("category")["views"].mean().sort_values(ascending=False)

#en çok izlenen %10 luk dilim

# views sütunu sayısal değilse düzelt
df["views"] = pd.to_numeric(df["views"], errors="coerce")

# En çok izlenen %10
threshold = df["views"].quantile(0.90)
top_df = df[df["views"] >= threshold]

print("Eşik (top %10):", threshold)
print("Video sayısı:", len(top_df))

#başlıklarda en çok kullanılan kelimeler
import re
from collections import Counter

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-zA-Zçğıöşü0-9\s]", "", text)
    return text

words = []

for title in top_df["title"]:
    cleaned = clean_text(title)
    words.extend(cleaned.split())

counter = Counter(words)
counter.most_common(20)

#hangi kelime daha çok izlenme getiriyor
from collections import defaultdict

word_views = defaultdict(list)

for _, row in df.iterrows():
    title = clean_text(row["title"])
    view = row["views"]
    for word in title.split():
        if len(word) > 3:
            word_views[word].append(view)

avg_word_views = {word: np.mean(views) for word, views in word_views.items() if len(views) > 20}

sorted(avg_word_views.items(), key=lambda x: x[1], reverse=True)[:20]